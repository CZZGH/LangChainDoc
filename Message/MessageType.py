'''
Message types
 System message - Tells the model how to behave and provide context for interactions
 Human message - Represents user input and interactions with the model
 AI message - Responses generated by the model, including text content, tool calls, and metadata
 Tool message - Represents the outputs of tool calls
'''
from langchain.chat_models import init_chat_model
from langchain.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from my_llm import create_llm

model = create_llm("deepseek-v3.2")


#1 system message
system_msg = SystemMessage("""
You are a senior Python developer with expertise in web frameworks.
Always provide code examples and explain your reasoning.
Be concise but thorough in your explanations.
""")

messages = [
    system_msg,
    HumanMessage("How do I create a REST API?")
]
#response = model.invoke(messages)

#2 Human Message
# response = model.invoke([
#   HumanMessage("What is machine learning?")
# ])


#3 AI Message
'''AIMessage ä»£è¡¨æ¨¡å‹è°ƒç”¨çš„è¾“å‡ºã€‚
å®ƒä»¬å¯ä»¥åŒ…å«å¤šæ¨¡æ€æ•°æ®ã€å·¥å…·è°ƒç”¨ä»¥åŠä½ ç¨åå¯ä»¥è®¿é—®çš„ç‰¹å®šäºæä¾›å•†çš„å…ƒæ•°æ®ã€‚'''
# Create an AI message manually (e.g., for conversation history)
ai_msg = AIMessage("I'd be happy to help you with that question!")

# Add to conversation history
messages = [
    SystemMessage("You are a helpful assistant"),
    HumanMessage("Can you help me?"),
    ai_msg,  # Insert as if it came from the model
    HumanMessage("Great! What's 2+2?")
]
#response = model.invoke(messages)


# Tool calls
def get_weather(location: str) -> str:
    """Get the weather at a location."""
    return f"It's sunny in {location}."

#model_with_tools = model.bind_tools([get_weather])
# response = model_with_tools.invoke("What's the weather in Paris?")

# for tool_call in response.tool_calls:
#     print(f"Tool: {tool_call['name']}")
#     print(f"Args: {tool_call['args']}")
#     print(f"ID: {tool_call['id']}")

# Token usage

# response = model.invoke("Hello!")
# print(response.usage_metadata)


'''
output:
{'input_tokens': 6, 'output_tokens': 39, 'total_tokens': 45, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}
ä½ å¥½ï¼ğŸ‘‹ å¾ˆé«˜å…´è§åˆ°ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€èŠå¤©ï¼Œè¿˜æ˜¯ååŠ©è§£å†³é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©ã€‚è¯·éšæ—¶å‘Šè¯‰æˆ‘ä½ éœ€è¦ä»€ä¹ˆï¼ğŸ˜Š
'''

# Streaming and chunks
chunks = []
full_message = None
# é€ä¸ªTokenæ‰“å°
# for chunk in model.stream("Hi"):
#     chunks.append(chunk)
#     print(chunk.text)
#     full_message = chunk if full_message is None else full_message + chunk


#4 Tool Message
# ----------- After a model makes a tool call
# (Here, we demonstrate manually creating the messages for brevity)
ai_message = AIMessage(
    content=[],
    tool_calls=[{
        "name": "get_weather",
        "args": {"location": "San Francisco"},
        "id": "call_123"
    }]
)

# Execute tool and create result message
weather_result = "Sunny, 72Â°F"
tool_message = ToolMessage(
    content=weather_result,
    tool_call_id="call_123"  # Must match the call ID
)

# # Continue conversation
# messages = [
#     HumanMessage("What's the weather in San Francisco?"),
#     ai_message,  # Model's tool call
#     tool_message,  # Tool execution result
# ]
# response = model.invoke(messages)  # Model processes the result



#--------------- Sent to model
message_content = "It was the best of times, it was the worst of times."

# Artifact available downstream
artifact = {"document_id": "doc_123", "page": 0}

tool_message = ToolMessage(
    content=message_content,
    tool_call_id="call_123",
    name="search_books",
    artifact=artifact,
)

messages = [
    HumanMessage("What's the weather in San Francisco?"),
    ai_message,  # Model's tool call
    tool_message,  # Tool execution result
]
response = model.invoke(messages)  # Model processes the result

print(response.content)



